{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import uuid\n",
    "import kfp\n",
    "\n",
    "import kfp.v2.dsl as dsl\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "sys.path.append('pipelines')\n",
    "from pipelines.pipeline import taxi_tip_predictor_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-workshop-bucket'\n",
    "REGION = 'us-central1'\n",
    "PIPELINES_SA = 'pipelines-sa@jk-mlops-dev.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def prepare_data_splits_op(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    \n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits.\"\"\"\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TABLE` \n",
    "    AS (\n",
    "        WITH\n",
    "        taxitrips AS (\n",
    "        SELECT\n",
    "            FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "        FROM\n",
    "            `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE 1=1 \n",
    "        AND pickup_longitude IS NOT NULL\n",
    "        AND pickup_latitude IS NOT NULL\n",
    "        AND dropoff_longitude IS NOT NULL\n",
    "        AND dropoff_latitude IS NOT NULL\n",
    "        AND trip_miles > 0\n",
    "        AND trip_seconds > 0\n",
    "        AND fare > 0\n",
    "        AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        trip_start_timestamp,\n",
    "        EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "        EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "        ) AS pickup_grid,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "        ) AS dropoff_grid,\n",
    "        ST_Distance(\n",
    "            ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "            ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "        ) AS euclidean,\n",
    "        IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "            WHEN 9 THEN 'testing'\n",
    "            WHEN 8 THEN 'validation'\n",
    "            ELSE 'training' END AS data_split\n",
    "        FROM\n",
    "        taxitrips\n",
    "        LIMIT @LIMIT\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    dataset.metadata['training_split'] = 'jk-mlops-dev.chicago_taxi_training.training_split'\n",
    "    dataset.metadata['validation_split'] = 'jk-mlops-dev.chicago_taxi_training.validation_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def train_op(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    epochs: int,\n",
    "    per_replica_batch_size: int,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "   \n",
    "):\n",
    "    \"\"\"Prepares and submits Vertex AI Training custom container job.\"\"\"\n",
    "    \n",
    "    CONTAINER_IMAGE_URI = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "    \n",
    "    import logging\n",
    "    import time\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "\n",
    "    # Set base_output_dir\n",
    "    if model.path[0:4] != '/gcs':\n",
    "        raise RuntimeError('Model dir must be a GCS location.')   \n",
    "    model_path = model.path.rsplit('/', 1)\n",
    "    if model_path[1] == 'model':\n",
    "        base_output_dir = model_path[0]\n",
    "    else:\n",
    "        base_output_dir = model_path\n",
    "    base_output_dir = 'gs://' + base_output_dir.split('/', 2)[2]\n",
    "\n",
    "    # Prepare worker pool specification\n",
    "    worker_pool_specs =  [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": CONTAINER_IMAGE_URI,\n",
    "                \"command\": [\"python\", \"train.py\"],\n",
    "                \"args\": [\n",
    "                    '--epochs=' + str(epochs), \n",
    "                    '--per_replica_batch_size=' + str(per_replica_batch_size),\n",
    "                    '--training_table=' + dataset.metadata['training_split'],\n",
    "                    '--validation_table=' + dataset.metadata['validation_split'],\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=region\n",
    "    )\n",
    "                                             \n",
    "    job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=base_output_dir\n",
    "    )\n",
    "\n",
    "    response = job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def prepare_dataset_op(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    dataset.metadata['training_split'] = 'jk-mlops-dev.chicago_taxi_training.training_split'\n",
    "    dataset.metadata['validation_split'] = 'jk-mlops-dev.chicago_taxi_training.validation_split'\n",
    "\n",
    "\n",
    "@component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def train_op(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    epochs: int,\n",
    "    per_replica_batch_size: int,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "   \n",
    "):\n",
    "    \n",
    "    CONTAINER_IMAGE_URI = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "    \n",
    "    import time\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=region\n",
    "    )\n",
    "    \n",
    "    job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    \n",
    "    print('************************')\n",
    "    print(model.path)\n",
    "    \n",
    "    \n",
    "    # Set base_output_dir\n",
    "    if model.path[0:4] != '/gcs':\n",
    "        raise RuntimeError('Model dir must be a GCS location.')   \n",
    "    model_path = model.path.rsplit('/', 1)\n",
    "    if model_path[1] == 'model':\n",
    "        base_output_dir = model_path[0]\n",
    "    else:\n",
    "        base_output_dir = model_path\n",
    "    base_output_dir = 'gs://' + base_output_dir.split('/', 2)[2]\n",
    "    \n",
    "    print('******************')\n",
    "    print(base_output_dir)\n",
    "        \n",
    "    \n",
    "    worker_pool_specs =  [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": CONTAINER_IMAGE_URI,\n",
    "                \"command\": [\"python\", \"train.py\"],\n",
    "                \"args\": [\n",
    "                    '--epochs=' + str(epochs), \n",
    "                    '--per_replica_batch_size=' + str(per_replica_batch_size),\n",
    "                    '--training_table=' + dataset.metadata['training_split'],\n",
    "                    '--validation_table=' + dataset.metadata['validation_split'],\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print('########################')\n",
    "    print(worker_pool_specs)\n",
    "    print(base_output_dir)\n",
    "    print(machine_type)\n",
    "    print('########################')\n",
    "                                                \n",
    "    \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=base_output_dir\n",
    "    )\n",
    "\n",
    "    response = job.run(sync=True)\n",
    "    \n",
    "\n",
    "@component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def test_op(\n",
    "    input1: Input[Model]\n",
    "):\n",
    "    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "    print(input1)\n",
    "    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_TRAINING_JOB_NAME = 'taxi-tip-predictor-training-job'\n",
    "PIPELINE_NAME = 'taxi-tip-predictor-continuous-training'\n",
    "\n",
    "@kfp.dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    staging_bucket: str,\n",
    "    epochs: int,\n",
    "    per_replica_batch_size: int,\n",
    "    training_table: str,\n",
    "    validation_table: str,\n",
    "    machine_type: str = 'n1-standard-4',\n",
    "    accelerator_type: str = 'NVIDIA_TESLA_T4',\n",
    "    accelerator_count: int = 1,\n",
    "):\n",
    "    \n",
    "    prepare_data = prepare_dataset_op(\n",
    "        project=project,\n",
    "        region=region\n",
    "    )\n",
    "    \n",
    "    train = train_op(\n",
    "        project=project,\n",
    "        region=region,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "        epochs=epochs,\n",
    "        per_replica_batch_size=per_replica_batch_size,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count\n",
    "    )\n",
    "    \n",
    "    test = test_op(\n",
    "        input1 = train.outputs['model']\n",
    "    )\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_pipeline,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tip-predictor-continuous-training-20210607171835?project=jk-mlops-dev\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_root = f'{STAGING_BUCKET}/pipelines'\n",
    "model_display_name = 'Taxi tip predictor'\n",
    "training_container_image = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "epochs = 3\n",
    "per_replica_batch_size = 128\n",
    "training_table = 'jk-mlops-dev.chicago_taxi_training.training_split'\n",
    "validation_table = 'jk-mlops-dev.chicago_taxi_training.validation_split'\n",
    "\n",
    "\n",
    "parameter_values = {\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'staging_bucket': STAGING_BUCKET,\n",
    "    'epochs': epochs,\n",
    "    'per_replica_batch_size': per_replica_batch_size,\n",
    "    'training_table': training_table,\n",
    "    'validation_table': validation_table,\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=parameter_values,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
