{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Vertex AI pipelines with the KFP v2 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import kfp\n",
    "import kfp.v2.dsl as dsl\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, Metrics, ClassificationMetrics)\n",
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.DEFINE_string('schema_file', None, 'Location of the data schema file')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "flags.mark_flag_as_required('schema_file')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "TARGET_TAG = 'target'\n",
    "\n",
    "\n",
    "def schema_to_features(schema):\n",
    "    \"\"\"Converts a schema_pb2 protobuf to feature dictionary.\"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    for feature in schema.feature:\n",
    "        if feature.type == 2:\n",
    "            if feature.int_domain.is_categorical:\n",
    "                features[feature.name] = ('categorical', tf.int64)\n",
    "            else:\n",
    "                features[feature.name] = ('numeric', tf.int64)\n",
    "        elif feature.type == 1:\n",
    "            features[feature.name] = ('categorical', tf.string)\n",
    "        elif feature.type == 3:\n",
    "            features[feature.name] = ('numeric', tf.double)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_target_feature(schema):\n",
    "    \"\"\"Returns the name of the target feature in schema.\"\"\"\n",
    "    \n",
    "    target_feature = None\n",
    "    for feature in schema.feature:\n",
    "        if feature.HasField('annotation'):\n",
    "            if TARGET_TAG in feature.annotation.tag:\n",
    "                target_feature = feature.name\n",
    "    return target_feature\n",
    "\n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, features, target_feature, batch_size=32):\n",
    "    \"\"\"Creates a tf.data dataset for direct access to BQ table.\"\"\"\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "    \n",
    "    selected_fields = {key: {'output_type': value[1]} \n",
    "                       for key, value in features.items()}\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Extract features from schema_pb2\n",
    "    schema = tfdv.load_schema_text(FLAGS.schema_file)\n",
    "    features = schema_to_features(schema)\n",
    "    target_feature = get_target_feature(schema)\n",
    "\n",
    "    if not target_feature:\n",
    "        raise RuntimeError('Schema does not have a target feature')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   features,\n",
    "                                   target_feature,\n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 features,\n",
    "                                 target_feature,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Prepare the model\n",
    "    logging.info('Creating the model ...')\n",
    "    input_features = {key: value for key, value in features.items() if key != target_feature}\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, FLAGS.units, FLAGS.dropout_ratio)\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure Keras callbacks\n",
    "    model_dir, tb_dir, checkpoint_dir = set_job_dirs()\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/taxi_classifier_trainer_v2'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 8.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-mlops-dev_cloudbuild/source/1623212966.202048-80f38d54ab8140d18695d810286612f8.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/locations/global/builds/0a592596-50da-47f3-9c54-5753e22cf9ab].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/0a592596-50da-47f3-9c54-5753e22cf9ab?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0a592596-50da-47f3-9c54-5753e22cf9ab\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1623212966.202048-80f38d54ab8140d18695d810286612f8.tgz#1623212966459818\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1623212966.202048-80f38d54ab8140d18695d810286612f8.tgz#1623212966459818...\n",
      "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
      "Operation completed over 1 objects/2.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-4\n",
      "6e0aa5e7af40: Pulling fs layer\n",
      "d47239a868b3: Pulling fs layer\n",
      "49cbb10cca85: Pulling fs layer\n",
      "4450dd082e0f: Pulling fs layer\n",
      "2c645f69b1d6: Pulling fs layer\n",
      "c51f99ae46a1: Pulling fs layer\n",
      "ece22f1f3a5c: Pulling fs layer\n",
      "0ecbf0b7bbf8: Pulling fs layer\n",
      "5848f1cb3b81: Pulling fs layer\n",
      "2f54b1f16acd: Pulling fs layer\n",
      "16e84ce81d85: Pulling fs layer\n",
      "c8e5550a4fa4: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "8c4a122297af: Pulling fs layer\n",
      "0a3be6fbb919: Pulling fs layer\n",
      "bd37ff43c32f: Pulling fs layer\n",
      "1200922938c4: Pulling fs layer\n",
      "c9544d041b64: Pulling fs layer\n",
      "3ba6aa913af7: Pulling fs layer\n",
      "14f071df3ff5: Pulling fs layer\n",
      "a70527b0d158: Pulling fs layer\n",
      "e83a95442471: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "83550171dabb: Pulling fs layer\n",
      "e7357577046f: Pulling fs layer\n",
      "e39246da14f5: Pulling fs layer\n",
      "792c46c3aca1: Pulling fs layer\n",
      "86b48621d244: Pulling fs layer\n",
      "b7db915d224a: Pulling fs layer\n",
      "c8e5550a4fa4: Waiting\n",
      "8c4a122297af: Waiting\n",
      "0a3be6fbb919: Waiting\n",
      "bd37ff43c32f: Waiting\n",
      "1200922938c4: Waiting\n",
      "c9544d041b64: Waiting\n",
      "3ba6aa913af7: Waiting\n",
      "14f071df3ff5: Waiting\n",
      "a70527b0d158: Waiting\n",
      "e83a95442471: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "83550171dabb: Waiting\n",
      "e7357577046f: Waiting\n",
      "e39246da14f5: Waiting\n",
      "792c46c3aca1: Waiting\n",
      "86b48621d244: Waiting\n",
      "b7db915d224a: Waiting\n",
      "424e7c9d5d89: Waiting\n",
      "4450dd082e0f: Waiting\n",
      "2c645f69b1d6: Waiting\n",
      "c51f99ae46a1: Waiting\n",
      "ece22f1f3a5c: Waiting\n",
      "0ecbf0b7bbf8: Waiting\n",
      "5848f1cb3b81: Waiting\n",
      "2f54b1f16acd: Waiting\n",
      "16e84ce81d85: Waiting\n",
      "d47239a868b3: Verifying Checksum\n",
      "d47239a868b3: Download complete\n",
      "49cbb10cca85: Download complete\n",
      "4450dd082e0f: Verifying Checksum\n",
      "4450dd082e0f: Download complete\n",
      "6e0aa5e7af40: Verifying Checksum\n",
      "6e0aa5e7af40: Download complete\n",
      "c51f99ae46a1: Verifying Checksum\n",
      "c51f99ae46a1: Download complete\n",
      "2c645f69b1d6: Verifying Checksum\n",
      "2c645f69b1d6: Download complete\n",
      "5848f1cb3b81: Verifying Checksum\n",
      "5848f1cb3b81: Download complete\n",
      "ece22f1f3a5c: Verifying Checksum\n",
      "ece22f1f3a5c: Download complete\n",
      "16e84ce81d85: Download complete\n",
      "6e0aa5e7af40: Pull complete\n",
      "d47239a868b3: Pull complete\n",
      "49cbb10cca85: Pull complete\n",
      "4450dd082e0f: Pull complete\n",
      "2c645f69b1d6: Pull complete\n",
      "c51f99ae46a1: Pull complete\n",
      "ece22f1f3a5c: Pull complete\n",
      "2f54b1f16acd: Verifying Checksum\n",
      "2f54b1f16acd: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "0ecbf0b7bbf8: Verifying Checksum\n",
      "0ecbf0b7bbf8: Download complete\n",
      "c8e5550a4fa4: Verifying Checksum\n",
      "c8e5550a4fa4: Download complete\n",
      "bd37ff43c32f: Verifying Checksum\n",
      "bd37ff43c32f: Download complete\n",
      "1200922938c4: Verifying Checksum\n",
      "1200922938c4: Download complete\n",
      "0a3be6fbb919: Verifying Checksum\n",
      "0a3be6fbb919: Download complete\n",
      "3ba6aa913af7: Verifying Checksum\n",
      "3ba6aa913af7: Download complete\n",
      "14f071df3ff5: Download complete\n",
      "a70527b0d158: Verifying Checksum\n",
      "a70527b0d158: Download complete\n",
      "e83a95442471: Verifying Checksum\n",
      "e83a95442471: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "8c4a122297af: Verifying Checksum\n",
      "8c4a122297af: Download complete\n",
      "c9544d041b64: Download complete\n",
      "e39246da14f5: Verifying Checksum\n",
      "e39246da14f5: Download complete\n",
      "83550171dabb: Verifying Checksum\n",
      "83550171dabb: Download complete\n",
      "86b48621d244: Verifying Checksum\n",
      "86b48621d244: Download complete\n",
      "b7db915d224a: Download complete\n",
      "792c46c3aca1: Verifying Checksum\n",
      "792c46c3aca1: Download complete\n",
      "e7357577046f: Verifying Checksum\n",
      "e7357577046f: Download complete\n",
      "0ecbf0b7bbf8: Pull complete\n",
      "5848f1cb3b81: Pull complete\n",
      "2f54b1f16acd: Pull complete\n",
      "16e84ce81d85: Pull complete\n",
      "c8e5550a4fa4: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "8c4a122297af: Pull complete\n",
      "0a3be6fbb919: Pull complete\n",
      "bd37ff43c32f: Pull complete\n",
      "1200922938c4: Pull complete\n",
      "c9544d041b64: Pull complete\n",
      "3ba6aa913af7: Pull complete\n",
      "14f071df3ff5: Pull complete\n",
      "a70527b0d158: Pull complete\n",
      "e83a95442471: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "83550171dabb: Pull complete\n",
      "e7357577046f: Pull complete\n",
      "e39246da14f5: Pull complete\n",
      "792c46c3aca1: Pull complete\n",
      "86b48621d244: Pull complete\n",
      "b7db915d224a: Pull complete\n",
      "Digest: sha256:6ddd47796c8223fa888f2e8ac2454e0ac49f1a36bbbc87d4c126d0eaba009c46\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-4:latest\n",
      " ---> ab93ebea3c35\n",
      "Step 2/4 : WORKDIR /trainer\n",
      " ---> Running in 2f38d56f0796\n",
      "Removing intermediate container 2f38d56f0796\n",
      " ---> aa5d55643d15\n",
      "Step 3/4 : COPY train.py .\n",
      " ---> 43b336e5ddf1\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 2a391fdfbb80\n",
      "Removing intermediate container 2a391fdfbb80\n",
      " ---> d9907487d343\n",
      "Successfully built d9907487d343\n",
      "Successfully tagged gcr.io/jk-mlops-dev/taxi_classifier_trainer_v2:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/taxi_classifier_trainer_v2\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/taxi_classifier_trainer_v2]\n",
      "cc571d5e6c34: Preparing\n",
      "02ac72068d9b: Preparing\n",
      "809d7b2f60d1: Preparing\n",
      "0643eb2eb480: Preparing\n",
      "40a2c3eb7ae8: Preparing\n",
      "79431f0fa8db: Preparing\n",
      "29591854aed3: Preparing\n",
      "d3263f816411: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "f6c5f07e787b: Preparing\n",
      "25fabe96190a: Preparing\n",
      "f5434bbff46e: Preparing\n",
      "a3349cefae00: Preparing\n",
      "17330ad88149: Preparing\n",
      "ae593ab21099: Preparing\n",
      "ac0c09736a4b: Preparing\n",
      "af04e844d06f: Preparing\n",
      "fc6b54c6ced7: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "d5de0a9a6a11: Preparing\n",
      "75867e8b38e6: Preparing\n",
      "edb28f196cf4: Preparing\n",
      "463a01dbc7de: Preparing\n",
      "716331d2d72b: Preparing\n",
      "c79fa966f459: Preparing\n",
      "64d8b9e63cdf: Preparing\n",
      "988749f5bf51: Preparing\n",
      "2d4faa2fa9fe: Preparing\n",
      "6f15325cc380: Preparing\n",
      "1e77dd81f9fa: Preparing\n",
      "030309cad0ba: Preparing\n",
      "ac0c09736a4b: Waiting\n",
      "af04e844d06f: Waiting\n",
      "fc6b54c6ced7: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "79431f0fa8db: Waiting\n",
      "d5de0a9a6a11: Waiting\n",
      "75867e8b38e6: Waiting\n",
      "29591854aed3: Waiting\n",
      "edb28f196cf4: Waiting\n",
      "d3263f816411: Waiting\n",
      "463a01dbc7de: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "716331d2d72b: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "f6c5f07e787b: Waiting\n",
      "25fabe96190a: Waiting\n",
      "f5434bbff46e: Waiting\n",
      "a3349cefae00: Waiting\n",
      "17330ad88149: Waiting\n",
      "ae593ab21099: Waiting\n",
      "c79fa966f459: Waiting\n",
      "64d8b9e63cdf: Waiting\n",
      "988749f5bf51: Waiting\n",
      "2d4faa2fa9fe: Waiting\n",
      "6f15325cc380: Waiting\n",
      "1e77dd81f9fa: Waiting\n",
      "030309cad0ba: Waiting\n",
      "809d7b2f60d1: Layer already exists\n",
      "0643eb2eb480: Layer already exists\n",
      "40a2c3eb7ae8: Layer already exists\n",
      "29591854aed3: Layer already exists\n",
      "d3263f816411: Layer already exists\n",
      "79431f0fa8db: Layer already exists\n",
      "b34dae69fc5d: Layer already exists\n",
      "06a5bf49b163: Layer already exists\n",
      "0ffb7465dde9: Layer already exists\n",
      "e2563d1ada9a: Layer already exists\n",
      "f6c5f07e787b: Layer already exists\n",
      "25fabe96190a: Layer already exists\n",
      "f5434bbff46e: Layer already exists\n",
      "17330ad88149: Layer already exists\n",
      "a3349cefae00: Layer already exists\n",
      "ae593ab21099: Layer already exists\n",
      "af04e844d06f: Layer already exists\n",
      "ac0c09736a4b: Layer already exists\n",
      "fc6b54c6ced7: Layer already exists\n",
      "5b9e34b5cf74: Layer already exists\n",
      "d5de0a9a6a11: Layer already exists\n",
      "75867e8b38e6: Layer already exists\n",
      "463a01dbc7de: Layer already exists\n",
      "edb28f196cf4: Layer already exists\n",
      "716331d2d72b: Layer already exists\n",
      "c79fa966f459: Layer already exists\n",
      "64d8b9e63cdf: Layer already exists\n",
      "988749f5bf51: Layer already exists\n",
      "2d4faa2fa9fe: Layer already exists\n",
      "6f15325cc380: Layer already exists\n",
      "1e77dd81f9fa: Layer already exists\n",
      "030309cad0ba: Layer already exists\n",
      "02ac72068d9b: Pushed\n",
      "cc571d5e6c34: Pushed\n",
      "latest: digest: sha256:5ca2aebb0afc209f0a3bd1128009745fe27e28397a188f0c3a08af29b8e37982 size: 7455\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                    STATUS\n",
      "0a592596-50da-47f3-9c54-5753e22cf9ab  2021-06-09T04:29:26+00:00  6M13S     gs://jk-mlops-dev_cloudbuild/source/1623212966.202048-80f38d54ab8140d18695d810286612f8.tgz  gcr.io/jk-mlops-dev/taxi_classifier_trainer_v2 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom KFP components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def ingest_data_op(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    sample_size: int,\n",
    "    year: int,\n",
    "    dataset_name: str,\n",
    "    train_split_name: str,\n",
    "    valid_split_name: str,\n",
    "    test_split_name: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits\n",
    "    from Chicago taxi public dataset.\"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import exceptions\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE TEMP TABLE features \n",
    "    AS (\n",
    "        WITH\n",
    "        taxitrips AS (\n",
    "        SELECT\n",
    "            FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "        FROM\n",
    "            `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE 1=1 \n",
    "        AND pickup_longitude IS NOT NULL\n",
    "        AND pickup_latitude IS NOT NULL\n",
    "        AND dropoff_longitude IS NOT NULL\n",
    "        AND dropoff_latitude IS NOT NULL\n",
    "        AND trip_miles > 0\n",
    "        AND trip_seconds > 0\n",
    "        AND fare > 0\n",
    "        AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        trip_start_timestamp,\n",
    "        EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "        EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "        ) AS pickup_grid,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "        ) AS dropoff_grid,\n",
    "        ST_Distance(\n",
    "            ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "            ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "        ) AS euclidean,\n",
    "        IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "            WHEN 9 THEN 'TEST'\n",
    "            WHEN 8 THEN 'VALIDATE'\n",
    "            ELSE 'TRAIN' END AS data_split\n",
    "        FROM\n",
    "        taxitrips\n",
    "        LIMIT @LIMIT\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TRAIN';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='VALIDATE';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TEST';\n",
    "\n",
    "    DROP TABLE features;\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    ds = bigquery.Dataset(f'{project}.{dataset_name}')\n",
    "    ds.location = bq_location\n",
    "    try:\n",
    "        ds = client.create_dataset(ds, timeout=30)\n",
    "        logging.info(f'Created dataset: {project}.{dataset_name}')\n",
    "    except exceptions.Conflict:\n",
    "        logging.info(f'Dataset {project}.{dataset_name} already exists')\n",
    "        \n",
    "    sql_script = sql_script_template.replace(\n",
    "        '@PROJECT', project).replace(\n",
    "        '@DATASET', dataset_name).replace(\n",
    "        '@TRAIN_SPLIT', train_split_name).replace(\n",
    "        '@VALIDATE_SPLIT', valid_split_name).replace(\n",
    "        '@TEST_SPLIT', test_split_name).replace(\n",
    "        '@YEAR', str(year)).replace(\n",
    "        '@LIMIT', str(sample_size))\n",
    "\n",
    "    job = client.query(sql_script)\n",
    "    job.result()\n",
    "    \n",
    "    dataset.metadata[METADATA_TRAIN_SPLIT_KEY] = f'{project}.{dataset_name}.{train_split_name}'\n",
    "    dataset.metadata[METADATA_VALID_SPLIT_KEY] = f'{project}.{dataset_name}.{valid_split_name}'\n",
    "    dataset.metadata[METADATA_TEST_SPLIT_KEY] = f'{project}.{dataset_name}.{test_split_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest',\n",
    "               packages_to_install=['google-cloud-bigquery[bqstorage,pandas]'])\n",
    "def generate_stats_op(\n",
    "    project: str,\n",
    "    sample_percentage: int,\n",
    "    dataset: Input[Dataset],\n",
    "    stats: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics for the data splits \n",
    "    referenced in the input Dataset artifact.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    SELECT * \n",
    "    FROM @TABLE\n",
    "    TABLESAMPLE SYSTEM (@SAMPLE_PERC PERCENT)\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    for key in [METADATA_TRAIN_SPLIT_KEY, METADATA_VALID_SPLIT_KEY, METADATA_TEST_SPLIT_KEY]:\n",
    "        if key in dataset.metadata.keys():\n",
    "            sql_script = sql_script_template.replace(\n",
    "                '@TABLE', dataset.metadata[key]).replace(\n",
    "                '@SAMPLE_PERC', str(sample_percentage))\n",
    "            \n",
    "            df = client.query(sql_script).result().to_dataframe()\n",
    "    \n",
    "            stats_proto = tfdv.generate_statistics_from_dataframe(\n",
    "                dataframe=df,\n",
    "                stats_options=tfdv.StatsOptions(\n",
    "                    num_top_values=50\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            file_path = os.path.join(stats.path, key)\n",
    "            os.makedirs(file_path)\n",
    "            tfdv.write_stats_text(stats_proto, \n",
    "                                  os.path.join(file_path, STATS_FILE_NAME))\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def validate_stats_op(\n",
    "    project: str,\n",
    "    stats: Input[Artifact],\n",
    "    schema: Input[Artifact],\n",
    "    anomalies: Output[Artifact],  \n",
    ")-> NamedTuple(\n",
    "    'ValidOutputs',\n",
    "    [\n",
    "        ('anomalies_detected', str)\n",
    "    ]):\n",
    "    \"\"\"Validates statistices referenced in the input stats Artifact.\"\"\"\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    ANOMALIES_FILE_NAME = 'anomalies.pbtxt'\n",
    "    TRUE = 'true'\n",
    "    FALSE = 'false'\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    schema_proto = tfdv.load_schema_text(\n",
    "        input_path=schema.path\n",
    "    ) \n",
    "    \n",
    "    anomalies_detected = FALSE\n",
    "    for folder in os.listdir(stats.path):\n",
    "        stats_proto = tfdv.load_stats_text(\n",
    "            input_path=os.path.join(stats.path, folder, STATS_FILE_NAME)\n",
    "        )\n",
    "        \n",
    "        anomalies_proto = tfdv.validate_statistics(\n",
    "            statistics=stats_proto, \n",
    "            schema=schema_proto\n",
    "        )\n",
    "        \n",
    "        file_path = os.path.join(anomalies.path, folder)\n",
    "        os.makedirs(file_path)\n",
    "        file_path = os.path.join(file_path, ANOMALIES_FILE_NAME)\n",
    "        tfdv.write_anomalies_text(anomalies_proto, file_path)\n",
    "                                 \n",
    "        if anomalies_proto.anomaly_info:\n",
    "            anomalies_detected = TRUE\n",
    "            logging.info('Anomamlies detected: {}'.format(file_path))\n",
    "    \n",
    "    output = namedtuple('ValidOutputs', ['anomalies_detected'])\n",
    "    \n",
    "    return output(anomalies_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile time settings\n",
    "PIPELINE_NAME = 'taxi-tip-classifier-continuous-training-pipeline'\n",
    "\n",
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'chicago_taxi_ml'\n",
    "TRAINING_TABLE_NAME = 'training'\n",
    "VALIDATION_TABLE_NAME = 'validation'\n",
    "TESTING_TABLE_NAME = 'testing'\n",
    "SCHEMA = 'gs://jk-vertex-workshop-bucket/schema/schema.pbtxt'\n",
    "\n",
    "#TRAINING_CONTAINER_IMAGE = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer_v2'\n",
    "TRAINING_CONTAINER_IMAGE = TRAIN_IMAGE\n",
    "TRAINING_MACHINE_TYPE = 'n1-standard-4'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "ACCELERATOR_COUNT = 1\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "SERVING_CONTAINER_IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest'\n",
    "SERVING_MACHINE_TYPE = 'n1-standard-4'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_training(\n",
    "    model_display_name: str,\n",
    "    epochs: int,\n",
    "    staging_bucket: str,\n",
    "    per_replica_batch_size: int,\n",
    "    sample_percentage: int = 100,\n",
    "    year: int = 2020,\n",
    "    sample_size: int = 1000000,\n",
    "):\n",
    "    \n",
    "    import_schema = kfp.dsl.importer(\n",
    "        artifact_uri=SCHEMA,\n",
    "        artifact_class=Artifact,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    prepare_data = ingest_data_op(\n",
    "        project=PROJECT,\n",
    "        bq_location=BQ_LOCATION,\n",
    "        sample_size=sample_size,\n",
    "        year=year,\n",
    "        dataset_name=BQ_DATASET_NAME,\n",
    "        train_split_name=TRAINING_TABLE_NAME,\n",
    "        valid_split_name=VALIDATION_TABLE_NAME,\n",
    "        test_split_name=TESTING_TABLE_NAME,\n",
    "    )\n",
    "    \n",
    "    generate_stats = generate_stats_op(\n",
    "        project=PROJECT,\n",
    "        sample_percentage=sample_percentage,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "    )\n",
    "    \n",
    "    validate_stats = validate_stats_op(\n",
    "        project=PROJECT,\n",
    "        schema=import_schema.output,\n",
    "        stats=generate_stats.outputs['stats'],\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(validate_stats.outputs['anomalies_detected'] == 'false',\n",
    "                       name = 'Anomalies detected'):\n",
    "    \n",
    "        args = [\n",
    "            '--epochs', str(epochs),\n",
    "            '--per_replica_batch_size', str(per_replica_batch_size),\n",
    "            '--training_table', f'{PROJECT}.{BQ_DATASET_NAME}.{TRAINING_TABLE_NAME}',\n",
    "            '--validation_table',  f'{PROJECT}.{BQ_DATASET_NAME}.{VALIDATION_TABLE_NAME}',\n",
    "            '--schema_file', SCHEMA,\n",
    "        ]\n",
    "        \n",
    "        train = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            project=PROJECT,\n",
    "            location=REGION,\n",
    "            display_name=model_display_name,\n",
    "            model_display_name=model_display_name,\n",
    "            container_uri=TRAINING_CONTAINER_IMAGE,\n",
    "            args=args,\n",
    "            replica_count=REPLICA_COUNT,\n",
    "            accelerator_type=ACCELERATOR_TYPE,\n",
    "            accelerator_count=ACCELERATOR_COUNT,\n",
    "            staging_bucket=staging_bucket,\n",
    "            model_serving_container_image_uri=SERVING_CONTAINER_IMAGE,\n",
    "        )\n",
    "    \n",
    "        create_endpoint = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT,\n",
    "            display_name=model_display_name\n",
    "        )\n",
    "        create_endpoint.after(train)\n",
    "    \n",
    "        deploy_model = gcc_aip.ModelDeployOp(\n",
    "            project=PROJECT,\n",
    "            endpoint=create_endpoint.outputs['endpoint'],\n",
    "            model=train.outputs['model'],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            machine_type=SERVING_MACHINE_TYPE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_training,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tip-classifier-continuous-training-pipeline-20210610001857?project=jk-mlops-dev\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STAGING_BUCKET = 'gs://jk-vertex-workshop-bucket/pipeline_runs'\n",
    "PIPELINES_SA = 'pipelines-sa@jk-mlops-dev.iam.gserviceaccount.com'\n",
    "        \n",
    "parameter_values = {\n",
    "    'model_display_name': 'Taxi tip predictor',\n",
    "    'epochs': 2,\n",
    "    'per_replica_batch_size': 128,\n",
    "    'staging_bucket': STAGING_BUCKET,\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
