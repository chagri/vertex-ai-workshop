{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import uuid\n",
    "import kfp\n",
    "\n",
    "import kfp.v2.dsl as dsl\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-workshop-bucket'\n",
    "REGION = 'us-central1'\n",
    "PIPELINES_SA = 'pipelines-sa@jk-mlops-dev.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def prepare_data_splits_op(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    sample_size: int,\n",
    "    year: int,\n",
    "    name_prefix: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits.\"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import exceptions\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE TEMP TABLE features \n",
    "    AS (\n",
    "        WITH\n",
    "        taxitrips AS (\n",
    "        SELECT\n",
    "            FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "        FROM\n",
    "            `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE 1=1 \n",
    "        AND pickup_longitude IS NOT NULL\n",
    "        AND pickup_latitude IS NOT NULL\n",
    "        AND dropoff_longitude IS NOT NULL\n",
    "        AND dropoff_latitude IS NOT NULL\n",
    "        AND trip_miles > 0\n",
    "        AND trip_seconds > 0\n",
    "        AND fare > 0\n",
    "        AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        trip_start_timestamp,\n",
    "        EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "        EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "        ) AS pickup_grid,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "        ) AS dropoff_grid,\n",
    "        ST_Distance(\n",
    "            ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "            ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "        ) AS euclidean,\n",
    "        IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "            WHEN 9 THEN 'TEST'\n",
    "            WHEN 8 THEN 'VALIDATE'\n",
    "            ELSE 'TRAIN' END AS data_split\n",
    "        FROM\n",
    "        taxitrips\n",
    "        LIMIT @LIMIT\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TRAIN';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='VALIDATE';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TEST';\n",
    "\n",
    "    DROP TABLE features;\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    dataset_name = f'{name_prefix}_dataset'\n",
    "    ds = bigquery.Dataset(f'{project}.{dataset_name}')\n",
    "    ds.location = bq_location\n",
    "    try:\n",
    "        ds = client.create_dataset(ds, timeout=30)\n",
    "        logging.info(f'Created dataset: {project}.{dataset_name}')\n",
    "    except exceptions.Conflict:\n",
    "        logging.info(f'Dataset {project}.{dataset_name} already exists')\n",
    "        \n",
    "    train_split = f'{name_prefix}_train'\n",
    "    valid_split = f'{name_prefix}_valid'\n",
    "    test_split = f'{name_prefix}_test'\n",
    "    sql_script = sql_script_template.replace(\n",
    "        '@PROJECT', project).replace(\n",
    "        '@DATASET', dataset_name).replace(\n",
    "        '@TRAIN_SPLIT', train_split).replace(\n",
    "        '@VALIDATE_SPLIT', valid_split).replace(\n",
    "        '@TEST_SPLIT', test_split).replace(\n",
    "        '@YEAR', str(year)).replace(\n",
    "        '@LIMIT', str(sample_size))\n",
    "\n",
    "    job = client.query(sql_script)\n",
    "    job.result()\n",
    "    \n",
    "    dataset.metadata['training_split'] = f'{project}.{dataset_name}.{train_split}'\n",
    "    dataset.metadata['validation_split'] = f'{project}.{dataset_name}.{valid_split}'\n",
    "    dataset.metadata['testing_split'] = f'{project}.{dataset_name}.{test_split}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def generate_stats_op(\n",
    "    project: str,\n",
    "    dataset: Input[Dataset],\n",
    "    stats: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics from the data splits.\"\"\"\n",
    "    \n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    training_split_name = dataset.metadata['training_split']\n",
    "    \n",
    "    sql_script = f'''\n",
    "    SELECT * \n",
    "    FROM {training_split_name} \n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    df = client.query(sql_script).result().to_dataframe()\n",
    "    \n",
    "    stats = tfdv.generate_statistics_from_dataframe(\n",
    "        dataframe=df,\n",
    "        stats_options=tfdv.StatsOptions(\n",
    "            weight_feature=None,\n",
    "            sample_rate=1,\n",
    "            num_top_values=50\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    file_path = os.path.join(stats.path, 'train')\n",
    "    tfdv.write_stats_text(stats, file_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def validate_stats_op(\n",
    "    project: str,\n",
    "    stats: Input[Artifact],\n",
    "    schema: Input[Artifact],\n",
    "    anomalies: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics from the data splits.\"\"\"\n",
    "    \n",
    "    import tensorflow_data_validation as tfdv\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    print(stats.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def train_op(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    epochs: int,\n",
    "    per_replica_batch_size: int,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "   \n",
    ") -> NamedTuple(\n",
    "  'TrainOutputs',\n",
    "  [\n",
    "    ('artifacts_uri', str)\n",
    "  ]):\n",
    "    \"\"\"Prepares and submits Vertex AI Training custom container job.\"\"\"\n",
    "    \n",
    "    \n",
    "    CONTAINER_IMAGE_URI = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "    \n",
    "    import logging\n",
    "    import time\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    output = namedtuple('TrainOutputs', ['artifacts_uri'])\n",
    "    \n",
    "    return output('gs://jk-vertex-workshop-bucket/pipelines/895222332033/taxi-tip-predictor-continuous-training-20210607212717/train-op_2367310107252883456/model')\n",
    "    \n",
    "\n",
    "    # Set base_output_dir\n",
    "    if model.path[0:4] != '/gcs':\n",
    "        raise RuntimeError('Model dir must be a GCS location.')   \n",
    " \n",
    "    base_output_dir = 'gs://' + model.path[5:].rsplit('/', 1)[0]\n",
    "\n",
    "    # Prepare worker pool specification\n",
    "    worker_pool_specs =  [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": CONTAINER_IMAGE_URI,\n",
    "                \"command\": [\"python\", \"train.py\"],\n",
    "                \"args\": [\n",
    "                    '--epochs=' + str(epochs), \n",
    "                    '--per_replica_batch_size=' + str(per_replica_batch_size),\n",
    "                    '--training_table=' + dataset.metadata['training_split'],\n",
    "                    '--validation_table=' + dataset.metadata['validation_split'],\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=region\n",
    "    )\n",
    "                                             \n",
    "    job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=base_output_dir\n",
    "    )\n",
    "\n",
    "    response = job.run(sync=True)\n",
    "    \n",
    "    return (f'{base_output_dir}/model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_TRAINING_JOB_NAME = 'taxi-tip-predictor-training-job'\n",
    "PIPELINE_NAME = 'taxi-tip-predictor-continuous-training'\n",
    "\n",
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_display_name: str,\n",
    "    epochs: int,\n",
    "    per_replica_batch_size: int,\n",
    "    schema: str,\n",
    "    machine_type: str = 'n1-standard-4',\n",
    "    accelerator_type: str = 'NVIDIA_TESLA_T4',\n",
    "    accelerator_count: int = 1,\n",
    "    bq_location: str = 'US',\n",
    "    year: int = 2020,\n",
    "    sample_size: int = 1000000,\n",
    "    name_prefix: str = 'chicago_taxi_tips',\n",
    "    serving_container_image: str = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest\"\n",
    "):\n",
    "    \n",
    "    import_schema = kfp.dsl.importer(\n",
    "        artifact_uri=schema,\n",
    "        artifact_class=Artifact,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    prepare_data = prepare_data_splits_op(\n",
    "        project=project,\n",
    "        bq_location=bq_location,\n",
    "        sample_size=sample_size,\n",
    "        year=year,\n",
    "        name_prefix=name_prefix,\n",
    "    )\n",
    "    \n",
    "    generate_stats = generate_stats_op(\n",
    "        project=project,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "    )\n",
    "    \n",
    "    validate_stats = validate_stats_op(\n",
    "        project=project,\n",
    "        schema=import_schema.output,\n",
    "        stats=generate_stats.outputs['stats'],\n",
    "    )\n",
    "    \n",
    "    train = train_op(\n",
    "        project=project,\n",
    "        region=region,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "        epochs=epochs,\n",
    "        per_replica_batch_size=per_replica_batch_size,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count\n",
    "    )\n",
    "    \n",
    "    upload_model = gcc_aip.ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=train.outputs['artifacts_uri'],\n",
    "        serving_container_image_uri=serving_container_image\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_pipeline,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tip-predictor-continuous-training-20210607235011?project=jk-mlops-dev\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_root = f'{STAGING_BUCKET}/pipelines'\n",
    "model_display_name = 'Taxi tip predictor'\n",
    "training_container_image = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "epochs = 3\n",
    "per_replica_batch_size = 128\n",
    "training_table = 'jk-mlops-dev.chicago_taxi_training.training_split'\n",
    "validation_table = 'jk-mlops-dev.chicago_taxi_training.validation_split'\n",
    "schema = 'gs://jk-vertex-workshop-bucket/schema/schema.pbtxt'\n",
    "\n",
    "\n",
    "parameter_values = {\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'model_display_name': model_display_name,\n",
    "    'epochs': epochs,\n",
    "    'schema': schema,\n",
    "    'per_replica_batch_size': per_replica_batch_size,\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=parameter_values,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
