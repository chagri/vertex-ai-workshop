{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex AutoML.\n",
    "\n",
    "![Training pipeline](../images/automl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites and configure GCP authentication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-test-1002'\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'jtest2'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TABLE_NAME = 'features'\n",
    "BQ_LOCATION = 'US'\n",
    "SAMPLE_SIZE = 500000\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a BQ dataset to host the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = BQ_LOCATION\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a table with training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000000\n",
    "year = 2020\n",
    "\n",
    "sql_script_template = '''\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TABLE` \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'TEST'\n",
    "          WHEN 8 THEN 'VALIDATE'\n",
    "          ELSE 'TRAIN' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ")\n",
    "'''\n",
    "\n",
    "sql_script = sql_script_template.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TABLE', BQ_TABLE_NAME).replace(\n",
    "    '@YEAR', str(year)).replace(\n",
    "    '@LIMIT', str(sample_size))\n",
    "\n",
    "job = client.query(sql_script)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * EXCEPT (trip_start_timestamp)\n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \n",
    "'''\n",
    "df = client.query(sql_script).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tabular dataset in Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'Chicago taxi trips'\n",
    "bq_source_uri = f'bq://{PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}'\n",
    "\n",
    "filter = f'display_name=\"{display_name}\"'\n",
    "\n",
    "dataset = vertex_ai.TabularDataset.list(filter=filter)\n",
    "if not dataset:\n",
    "    print(\"Creating a new dataset.\")\n",
    "    dataset = vertex_ai.TabularDataset.create(\n",
    "        display_name=display_name, bq_source=bq_source_uri,\n",
    "    )\n",
    "\n",
    "    dataset.wait()\n",
    "else:\n",
    "    print(\"Using existing dataset: \", dataset[0].resource_name)\n",
    "    dataset = vertex_ai.TabularDataset(dataset_name=dataset[0].resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching an AutoML training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'Chicago Taxi classifier training'\n",
    "model_display_name = 'Chicago Taxi classifier'\n",
    "target_column = 'tip_bin'\n",
    "optimization_prediction_type = 'classification'\n",
    "optimization_objective = 'maximize-recall-at-precision'\n",
    "optimization_objective_precision_value = 0.7\n",
    "split_column = 'data_split'\n",
    "budget_milli_node_hours = 1000\n",
    "\n",
    "column_transformations = [\n",
    "    {'categorical': {'column_name': 'trip_month'}},\n",
    "    {'categorical': {'column_name': 'trip_day'}},\n",
    "    {'categorical': {'column_name': 'trip_day_of_week'}},\n",
    "    {'categorical': {'column_name': 'trip_hour'}},\n",
    "    {'categorical': {'column_name': 'payment_type'}},\n",
    "    {'categorical': {'column_name': 'pickup_grid'}},\n",
    "    {'categorical': {'column_name': 'dropoff_grid'}},\n",
    "    {'numeric': {'column_name': 'trip_seconds'}},\n",
    "    {'numeric': {'column_name': 'euclidean'}},\n",
    "    {'numeric': {'column_name': 'trip_miles'}},\n",
    "]\n",
    "\n",
    "job = vertex_ai.AutoMLTabularTrainingJob(\n",
    "    display_name=display_name,\n",
    "    optimization_prediction_type=optimization_prediction_type,\n",
    "    optimization_objective=optimization_objective,\n",
    "    optimization_objective_precision_value=optimization_objective_precision_value,\n",
    "    column_transformations=column_transformations,\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=target_column,\n",
    "    budget_milli_node_hours=budget_milli_node_hours,\n",
    "    model_display_name=model_display_name,\n",
    "    predefined_split_column_name=split_column,\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Platform_(Unified)_SDK_AutoML_Image_Classification_Training.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-cpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
