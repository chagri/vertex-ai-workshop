{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1da9f9",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e6e97",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e6afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io import bigquery as tfio_bq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39330cd",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites and configure GCP authentication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bf96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-workshop-bucket'\n",
    "VERTEX_SA = 'vertex-sa@jk-mlops-dev.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec2316",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ba75d",
   "metadata": {},
   "source": [
    "## Create or set Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bbccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_client = api_client = vertex_ai.initializer.global_config.create_client(\n",
    "        client_class=vertex_ai.utils.TensorboardClientWithOverride, location_override=REGION\n",
    ")\n",
    "parent = f'projects/{PROJECT}/locations/{REGION}'\n",
    "\n",
    "tensorboard_display_name = 'Workshop Tensorboard'\n",
    "tensorboard_ref = None\n",
    "\n",
    "for tensorboard in tb_client.list_tensorboards(parent=parent):\n",
    "    if tensorboard.display_name == tensorboard_display_name:\n",
    "        tensorboard_ref = tensorboard\n",
    "        \n",
    "if not tensorboard_ref:\n",
    "    print('Creating new Tensorboard')\n",
    "    tb_specs = types.Tensorboard(\n",
    "        display_name=tensorboard_display_name,\n",
    "        description=tensorboard_display_name\n",
    "    )\n",
    "    operation = tb_client.create_tensorboard(parent=parent, tensorboard=tb_specs)\n",
    "    tensorboard_ref = operation.result()\n",
    "else:\n",
    "    print('Using existing Tensorboard:', tensorboard_ref.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91a70a",
   "metadata": {},
   "source": [
    "## Prepare training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199ebff",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9682b",
   "metadata": {},
   "source": [
    "### Creating training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef602d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = 'training_dataset' # Change to your BQ datasent name.\n",
    "BQ_TABLE_NAME = 'training_table'\n",
    "BQ_LOCATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06201956",
   "metadata": {},
   "source": [
    "#### Create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376a5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jk-mlops-dev.training_dataset already exists\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = BQ_LOCATION\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986cf50",
   "metadata": {},
   "source": [
    "#### Create a training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000000\n",
    "year = 2020\n",
    "\n",
    "sql_script = '''\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TABLE` \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'testing'\n",
    "          WHEN 8 THEN 'validation'\n",
    "          ELSE 'training' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ")\n",
    "'''\n",
    "\n",
    "sql_script = sql_script.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TABLE', BQ_TABLE_NAME).replace(\n",
    "    '@YEAR', str(year)).replace(\n",
    "    '@LIMIT', str(sample_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fe600",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.query(sql_script)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \n",
    "WHERE data_split='training'\n",
    "'''\n",
    "training_data = client.query(sql_script).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT ARRAY(SELECT DISTINCT payment_type FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}) as payment_type_vocab,\n",
    "       ARRAY(SELECT DISTINCT pickup_grid FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}) as pickup_grid_vocab,\n",
    "       ARRAY(SELECT DISTINCT dropoff_grid FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}) as dropoff_grid_vocab,\n",
    "       (SELECT AS STRUCT AVG(trip_seconds), VARIANCE(trip_seconds) FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}) as trip_seconds_stats,\n",
    "'''\n",
    "\n",
    "training_data_stats = client.query(sql_script).result().to_dataframe()\n",
    "training_data_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \n",
    "'''\n",
    "sample_data = client.query(sql_script).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213858b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.tip_bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.euclidean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \n",
    "WHERE data_split='training'\n",
    "'''\n",
    "training_data = client.query(sql_script).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d68d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data_split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cbbd7",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af075b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfio_bq_client = tfio_bq.BigQueryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42a8935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = \"tip_bin\"\n",
    "\n",
    "TARGET_LABELS = [\"tip<20%\", \"tip>=20%\"]\n",
    "\n",
    "\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", dtypes.int32),\n",
    "    \"trip_month\": (\"categorical\", dtypes.int32),\n",
    "    \"trip_day\": (\"categorical\", dtypes.int32),\n",
    "    \"trip_day_of_week\": (\"categorical\", dtypes.int32),\n",
    "    \"trip_hour\": (\"categorical\", dtypes.int32),\n",
    "    \"payment_type\": (\"categorical\", dtypes.string),\n",
    "    \"pickup_grid\": (\"categorical\", dtypes.string),\n",
    "    \"dropoff_grid\": (\"categorical\", dtypes.string),\n",
    "    \"euclidean\": (\"numeric\", dtypes.double),\n",
    "    \"trip_seconds\": (\"numeric\", dtypes.double),\n",
    "    \"trip_miles\": (\"numeric\", dtypes.double),\n",
    "}\n",
    "\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", dtypes.int64),\n",
    "    \"trip_month\": (\"categorical\", dtypes.int64),\n",
    "    \"trip_day\": (\"categorical\", dtypes.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", dtypes.int64),\n",
    "    \"trip_hour\": (\"categorical\", dtypes.int64),\n",
    "    \"payment_type\": (\"categorical\", dtypes.string),\n",
    "    \"pickup_grid\": (\"categorical\", dtypes.string),\n",
    "    \"dropoff_grid\": (\"categorical\", dtypes.string),\n",
    "    \"euclidean\": (\"numeric\", dtypes.double),\n",
    "    \"trip_seconds\": (\"numeric\", dtypes.int64),\n",
    "    \"trip_miles\": (\"numeric\", dtypes.double),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "102624bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tip_bin', 'trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'payment_type', 'pickup_grid', 'dropoff_grid', 'euclidean', 'trip_seconds', 'trip_miles')\n",
      "(tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.string, tf.string, tf.string, tf.float64, tf.int64, tf.float64)\n"
     ]
    }
   ],
   "source": [
    "selected_fields, output_types = zip(*[(key, value[1]) for key, value in FEATURES.items()])\n",
    "\n",
    "print(selected_fields)\n",
    "print(output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "305e58c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tip_bin': {'output_type': tf.int64},\n",
       " 'trip_month': {'output_type': tf.int64},\n",
       " 'trip_day': {'output_type': tf.int64},\n",
       " 'trip_day_of_week': {'output_type': tf.int64},\n",
       " 'trip_hour': {'output_type': tf.int64},\n",
       " 'payment_type': {'output_type': tf.string},\n",
       " 'pickup_grid': {'output_type': tf.string},\n",
       " 'dropoff_grid': {'output_type': tf.string},\n",
       " 'euclidean': {'output_type': tf.float64},\n",
       " 'trip_seconds': {'output_type': tf.int64},\n",
       " 'trip_miles': {'output_type': tf.float64}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "selected_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11d26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = f'projects/{PROJECT}'\n",
    "#selected_fields = ['trip_seconds', 'trip_miles']\n",
    "output_types = [dtypes.int64, dtypes.double]\n",
    "\n",
    "\n",
    "read_session = tfio_bq_client.read_session(\n",
    "    parent=parent,\n",
    "    project_id=PROJECT,\n",
    "    table_id=BQ_TABLE_NAME,\n",
    "    dataset_id=BQ_DATASET_NAME,\n",
    "    selected_fields=selected_fields,\n",
    "    #output_types=output_types\n",
    ")\n",
    "\n",
    "dataset = read_session.parallel_read_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68869b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10788bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dropoff_grid',\n",
       "              <tf.Tensor: shape=(), dtype=string, numpy=b'POINT(-87.6 41.8)'>),\n",
       "             ('euclidean',\n",
       "              <tf.Tensor: shape=(), dtype=float64, numpy=10666.168994769989>),\n",
       "             ('payment_type',\n",
       "              <tf.Tensor: shape=(), dtype=string, numpy=b'Unknown'>),\n",
       "             ('pickup_grid',\n",
       "              <tf.Tensor: shape=(), dtype=string, numpy=b'POINT(-87.7 41.9)'>),\n",
       "             ('tip_bin', <tf.Tensor: shape=(), dtype=int64, numpy=0>),\n",
       "             ('trip_day', <tf.Tensor: shape=(), dtype=int64, numpy=19>),\n",
       "             ('trip_day_of_week', <tf.Tensor: shape=(), dtype=int64, numpy=6>),\n",
       "             ('trip_hour', <tf.Tensor: shape=(), dtype=int64, numpy=0>),\n",
       "             ('trip_miles', <tf.Tensor: shape=(), dtype=float64, numpy=0.4>),\n",
       "             ('trip_month', <tf.Tensor: shape=(), dtype=int64, numpy=6>),\n",
       "             ('trip_seconds', <tf.Tensor: shape=(), dtype=int64, numpy=900>)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in dataset.take(5):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d49f76",
   "metadata": {},
   "source": [
    "## Create a Vertex managed tabular dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed93b2",
   "metadata": {},
   "source": [
    "### List existing tabular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'Chicago Taxi Tips'\n",
    "filter = 'display_name=\"{}\"'.format(display_name)\n",
    "\n",
    "for dataset in vertex_ai.TabularDataset.list(filter=filter):\n",
    "    print(dataset.display_name, ' : ', dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af3a9e",
   "metadata": {},
   "source": [
    "### Create a new tabular dataset based on the BigQuery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_source = f'bq://{PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}'\n",
    "\n",
    "\n",
    "dataset = vertex_ai.TabularDataset.create(\n",
    "    display_name=display_name, bq_source=bq_source\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820b408",
   "metadata": {},
   "source": [
    "## Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'trainer'\n",
    "if tf.io.gfile.exists(folder):\n",
    "    tf.io.gfile.rmtree(folder)\n",
    "tf.io.gfile.mkdir(folder)\n",
    "file_path = os.path.join(folder, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"trip_month\",\n",
    "    \"trip_day\",\n",
    "    \"trip_day_of_week\",\n",
    "    \"trip_hour\",\n",
    "    \"trip_seconds\",\n",
    "    \"trip_miles\",\n",
    "    \"payment_type\",\n",
    "    \"pickup_grid\",\n",
    "    \"dropoff_grid\",\n",
    "    \"euclidean\",\n",
    "    \"loc_cross\",\n",
    "]\n",
    "\n",
    "TARGET_FEATURE_NAME = \"tip_bin\"\n",
    "\n",
    "TARGET_LABELS = [\"tip<20%\", \"tip>=20%\"]\n",
    "\n",
    "NUMERICAL_FEATURE_NAMES = [\n",
    "    \"trip_seconds\",\n",
    "    \"trip_miles\",\n",
    "    \"euclidean\",\n",
    "]\n",
    "\n",
    "EMBEDDING_CATEGORICAL_FEATURES = {\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day\": 4,\n",
    "    \"trip_hour\": 3,\n",
    "    \"pickup_grid\": 3,\n",
    "    \"dropoff_grid\": 3,\n",
    "    \"loc_cross\": 10,\n",
    "}\n",
    "\n",
    "ONEHOT_CATEGORICAL_FEATURE_NAMES = [\"payment_type\", \"trip_day_of_week\"]\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for name in NUMERICAL_FEATURE_NAMES:\n",
    "        inputs[name] = keras.layers.Input(name=name, shape=[], dtype=tf.float32)\n",
    "    for name in list(EMBEDDING_CATEGORICAL_FEATURES.keys()) + ONEHOT_CATEGORICAL_FEATURE_NAMES:\n",
    "        inputs[name] = keras.layers.Input(name=name, shape=[], dtype=tf.int64)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_binary_classifier(feature_vocab_sizes, hyperparams):\n",
    "    input_layers = create_model_inputs()\n",
    "\n",
    "    layers = []\n",
    "    for feature_name in input_layers.keys:\n",
    "        if feature_name in EMBEDDING_CATEGORICAL_FEATURES:\n",
    "            vocab_size = feature_vocab_sizes[feature_name]\n",
    "            embedding_size = EMBEDDING_CATEGORICAL_FEATURES[feature_name]\n",
    "            embedding_output = keras.layers.Embedding(\n",
    "                input_dim=vocab_size + 1,\n",
    "                output_dim=embedding_size,\n",
    "                name=f\"{feature_name}_embedding\",\n",
    "            )(input_layers[feature_name])\n",
    "            layers.append(embedding_output)\n",
    "        elif feature_name in ONEHOT_CATEGORICAL_FEATURE_NAMES:\n",
    "            vocab_size = feature_vocab_sizes[feature_name]\n",
    "            onehot_layer = keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "                max_tokens=vocab_size,\n",
    "                output_mode=\"binary\",\n",
    "                name=f\"{feature_name}_onehot\",\n",
    "            )(input_layers[feature_name])\n",
    "            layers.append(onehot_layer)\n",
    "        elif feature_name in NUMERICAL_FEATURE_NAMES:\n",
    "            numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
    "            layers.append(numeric_layer)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    joined = keras.layers.Concatenate(name=\"combines_inputs\")(layers)\n",
    "    feedforward_output = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(units, activation=\"relu\")\n",
    "            for units in hyperparams[\"hidden_units\"]\n",
    "        ],\n",
    "        name=\"feedforward_network\",\n",
    "    )(joined)\n",
    "    logits = keras.layers.Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "\n",
    "    model = keras.Model(inputs=input_layers, outputs=[logits])\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    inputs = create_model_inputs()\n",
    "    print(inputs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bc9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
