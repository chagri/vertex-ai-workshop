{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995a6915",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e882d5",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites and configure GCP authentication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec431e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-vertex-workshop'\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'jkvw'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'{PREFIX}-training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "TENSORBOARD = 'projects/910094146258/locations/us-central1/tensorboards/6406405654306619392'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0f44c",
   "metadata": {},
   "source": [
    "## Preparing training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82ff31",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cc33a",
   "metadata": {},
   "source": [
    "### Create  data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "BQ_LOCATION = 'US'\n",
    "SAMPLE_SIZE = 500000\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec40505",
   "metadata": {},
   "source": [
    "#### Create a BQ dataset to host the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d91b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = BQ_LOCATION\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11332125",
   "metadata": {},
   "source": [
    "#### Create training, validation, and test splits tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script_template = '''\n",
    "CREATE TEMP TABLE features \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'TEST'\n",
    "          WHEN 8 THEN 'VALIDATE'\n",
    "          ELSE 'TRAIN' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TRAIN';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='VALIDATE';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TEST';\n",
    "\n",
    "DROP TABLE features;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef078fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = sql_script_template.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TRAIN_SPLIT', BQ_TRAIN_SPLIT_NAME).replace(\n",
    "    '@VALIDATE_SPLIT', BQ_VALID_SPLIT_NAME).replace(\n",
    "    '@TEST_SPLIT', BQ_TEST_SPLIT_NAME).replace(\n",
    "    '@YEAR', str(YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE))\n",
    "\n",
    "job = client.query(sql_script)\n",
    "job.result()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d0ca0",
   "metadata": {},
   "source": [
    "#### Review splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede36c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}\n",
    "'''\n",
    "\n",
    "data = client.query(sql_script).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3224f75",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffff1b8",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cdcc5f",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b34f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e88a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9a8ba",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff96bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ca294",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{STAGING_BUCKET}/jobs/{job_name}'\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    '--epochs=2', \n",
    "    '--per_replica_batch_size=128',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e81410",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b895c4c",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_taxi_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933db300",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7db2a6",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7524bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--epochs=2', \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6c725",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488da9e",
   "metadata": {},
   "source": [
    "### Configure and submit a hyperparameter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"HYPER_JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "hyperparameter_tuning_job_spec = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": 3,\n",
    "        \"parallel_trial_count\": 3,\n",
    "        \"max_failed_trial_count\": 1,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"metric_id\": \"val_accuracy\",\n",
    "                    \"goal\": vertex_ai.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": [\n",
    "                {\n",
    "                    \"parameter_id\": \"units\",\n",
    "                    \"discrete_value_spec\": {\"values\": [32, 64]},\n",
    "                },\n",
    "                {\n",
    "                    \"parameter_id\": \"dropout_ratio\",\n",
    "                    \"double_value_spec\": {\"min_value\": 0.4, \"max_value\": 0.6},\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"trial_job_spec\": {\n",
    "            \"base_output_directory\": {\n",
    "                \"output_uri_prefix\": f\"{STAGING_BUCKET}/{job_name}\"\n",
    "            },\n",
    "            \"service_account\": VERTEX_SA,\n",
    "            \"tensorboard\": TENSORBOARD,\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "                      #  \"accelerator_count\": 1,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": TRAIN_IMAGE,\n",
    "                        \"args\": [\n",
    "                            '--epochs=5', \n",
    "                            '--per_replica_batch_size=128',\n",
    "                            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0206855",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_client = api_client = vertex_ai.initializer.global_config.create_client(\n",
    "        client_class=JobClientWithOverride, location_override=REGION\n",
    ")\n",
    "\n",
    "parent = f'projects/{PROJECT}/locations/{REGION}'\n",
    "version = 'v1beta1'\n",
    "\n",
    "response = job_client.select_version(version).create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8ac48",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a866040",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_spec = job_client.get_hyperparameter_tuning_job(\n",
    "    name = response.name\n",
    ")\n",
    "print(job_spec.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fca0b4",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3466121",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    job_response = job_client.get_hyperparameter_tuning_job(\n",
    "        name = response.name)\n",
    "    if job_response.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", job_response.state)\n",
    "        if (job_response.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           job_response.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9e427",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = None\n",
    "for trial in job_response.trials:\n",
    "    accuracy = float(trial.final_measurement.metrics[0].value)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_trial = trial\n",
    "        best_accuracy = accuracy\n",
    "print(best_trial)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b56b12",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f'{STAGING_BUCKET}/{job_name}'\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id)\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d214f",
   "metadata": {},
   "source": [
    "## Deploying a model to Vertex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b797ee5",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b23ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = f'{best_model_dir}/model'\n",
    "#saved_model_path = 'gs://jk-vertex-workshop-bucket/models/taxi'\n",
    "\n",
    "!saved_model_cli show --dir {saved_model_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e7660",
   "metadata": {},
   "source": [
    "### Upload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe00299",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'Chicago taxi tip classifier'\n",
    "description = 'Chicago taxi tip TensorFlow classifier'\n",
    "serving_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest'\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    artifact_uri=saved_model_path,\n",
    "    serving_container_image_uri=serving_image_uri\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb27b9",
   "metadata": {},
   "source": [
    "### Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'Taxi tip classifier endpoint'\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da7944",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_display_name = 'taxi-v1'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2523f",
   "metadata": {},
   "source": [
    "## Invoking the deployed model using Vertex SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb7165",
   "metadata": {},
   "source": [
    "### Get the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4853777",
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint_info in vertex_ai.Endpoint.list(filter='display_name=\"Taxi tip classifier endpoint\"'):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8607d5",
   "metadata": {},
   "source": [
    "### Call the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3dcc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e6427",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d377",
   "metadata": {},
   "source": [
    "### Undeploy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3972fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc883691",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f3a0d",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d5484",
   "metadata": {},
   "source": [
    "### Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fb647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
